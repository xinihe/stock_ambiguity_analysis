{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity Metric on 2018-01-09: 0.005197856487538408\n",
      "Risk (Standard Deviation of Daily Returns) on 2018-01-09: 0.0017383918259038763\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "from typing import Tuple, List, Union  # For type hints\n",
    "from datetime import date  # For date handling\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the input DataFrame by calculating returns and extracting dates.\n",
    "    This function handles the initial data preprocessing steps.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'datetime' and 'close' columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'return' and 'date' columns, sorted by datetime\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime column to pandas datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Sort data chronologically to ensure correct return calculations\n",
    "    df.sort_values('datetime', inplace=True)\n",
    "    \n",
    "    # Calculate percentage returns (price changes)\n",
    "    # pct_change() computes the percentage change between consecutive elements\n",
    "    df['return'] = df['close'].pct_change()\n",
    "    \n",
    "    # Extract date component from datetime for daily grouping\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    # Remove rows with NaN returns (first row will have NaN as there's no previous price)\n",
    "    return df.dropna(subset=['return'])\n",
    "\n",
    "def get_window_dates(df: pd.DataFrame, specific_date: Union[str, date], \n",
    "                     window_size: int) -> List[date]:\n",
    "    \"\"\"\n",
    "    Get the dates for the rolling window analysis.\n",
    "    This function ensures we have enough data for the analysis and returns\n",
    "    the appropriate date range.\n",
    "    \n",
    "    Args:\n",
    "        df: Prepared DataFrame with 'date' column\n",
    "        specific_date: Target date for analysis\n",
    "        window_size: Number of days in rolling window\n",
    "        \n",
    "    Returns:\n",
    "        List of dates in the rolling window\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If specific_date not found or insufficient data\n",
    "    \"\"\"\n",
    "    # Convert specific_date to datetime.date object if it's a string\n",
    "    specific_date = pd.to_datetime(specific_date).date()\n",
    "    \n",
    "    # Get unique dates and sort them chronologically\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # Validate that the specific date exists in our data\n",
    "    if specific_date not in unique_dates:\n",
    "        raise ValueError(\"Specific date not found in the data.\")\n",
    "    \n",
    "    # Find the index of our target date\n",
    "    date_index = unique_dates.index(specific_date)\n",
    "    \n",
    "    # Check if we have enough historical data for the window\n",
    "    if date_index < window_size - 1:\n",
    "        raise ValueError(\"Not enough data for the specified window size.\")\n",
    "    \n",
    "    # Return the window of dates (including the specific date)\n",
    "    return unique_dates[date_index - window_size + 1 : date_index + 1]\n",
    "\n",
    "def calculate_daily_probabilities(window_data: pd.DataFrame, \n",
    "                                num_bins: int) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate probability distributions for each day in the window.\n",
    "    This function creates histograms of returns and converts them to probability distributions.\n",
    "    \n",
    "    Args:\n",
    "        window_data: DataFrame containing window data with 'return' and 'date' columns\n",
    "        num_bins: Number of bins for histogram (determines granularity of distribution)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - DataFrame of daily probability distributions\n",
    "        - Array of bin edges used for the histograms\n",
    "    \"\"\"\n",
    "    # Find the range of returns in our window\n",
    "    min_return = window_data['return'].min()\n",
    "    max_return = window_data['return'].max()\n",
    "    \n",
    "    # Create evenly spaced bins across the return range\n",
    "    bins = np.linspace(min_return, max_return, num_bins + 1)\n",
    "    \n",
    "    # Calculate probability distribution for each day\n",
    "    daily_probs = []\n",
    "    for date in window_data['date'].unique():\n",
    "        # Get returns for this specific day\n",
    "        daily_returns = window_data[window_data['date'] == date]['return']\n",
    "        \n",
    "        # Create histogram of returns\n",
    "        counts, _ = np.histogram(daily_returns, bins=bins)\n",
    "        \n",
    "        # Convert counts to probabilities\n",
    "        # If there are no returns (counts.sum() == 0), use zeros\n",
    "        probabilities = counts / counts.sum() if counts.sum() > 0 else np.zeros(num_bins)\n",
    "        daily_probs.append(probabilities)\n",
    "    \n",
    "    # Create DataFrame with dates as index and probabilities as columns\n",
    "    return pd.DataFrame(daily_probs, index=window_data['date'].unique()), bins\n",
    "\n",
    "def calculate_risk(df: pd.DataFrame, window_dates: List[date]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate risk as standard deviation of daily returns.\n",
    "    This is a traditional measure of volatility/risk in financial markets.\n",
    "    \n",
    "    Args:\n",
    "        df: Original DataFrame with price data\n",
    "        window_dates: List of dates in the analysis window\n",
    "        \n",
    "    Returns:\n",
    "        Risk metric (standard deviation of daily returns)\n",
    "    \"\"\"\n",
    "    # Get the last price of each day\n",
    "    daily_close = df.groupby('date')['close'].last()\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    daily_returns = daily_close.pct_change().dropna()\n",
    "    \n",
    "    # Filter returns for our window\n",
    "    window_daily_returns = daily_returns.loc[window_dates]\n",
    "    \n",
    "    # Calculate standard deviation of daily returns\n",
    "    return window_daily_returns.std()\n",
    "\n",
    "def calculate_ambiguity_and_risk(df: pd.DataFrame, \n",
    "                               specific_date: Union[str, date],\n",
    "                               window_size: int = 5, \n",
    "                               num_bins: int = 20) -> Tuple[float, pd.Series, float]:\n",
    "    \"\"\"\n",
    "    Main function to calculate ambiguity and risk metrics for a specific date.\n",
    "    This function orchestrates the entire analysis process.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'datetime' and 'close' columns\n",
    "        specific_date: Target date for analysis\n",
    "        window_size: Number of days in rolling window (default: 5)\n",
    "        num_bins: Number of bins for histogram (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - ambiguity_metric: Average standard deviation across bins\n",
    "        - interval_std: Standard deviations for each bin\n",
    "        - risk: Standard deviation of daily returns\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare the data\n",
    "    df = prepare_data(df)\n",
    "    \n",
    "    # Step 2: Get the dates for our analysis window\n",
    "    window_dates = get_window_dates(df, specific_date, window_size)\n",
    "    \n",
    "    # Step 3: Filter data for our window\n",
    "    window_data = df[df['date'].isin(window_dates)].copy()\n",
    "    \n",
    "    # Step 4: Calculate probability distributions\n",
    "    prob_df, _ = calculate_daily_probabilities(window_data, num_bins)\n",
    "    \n",
    "    # Step 5: Calculate ambiguity metrics\n",
    "    # Standard deviation across days for each bin\n",
    "    interval_std = prob_df.std(axis=0)\n",
    "    # Average standard deviation across all bins\n",
    "    daily_ambiguity_metric = interval_std.mean()\n",
    "    \n",
    "    # Step 6: Calculate risk\n",
    "    risk = calculate_risk(df, window_dates)\n",
    "    \n",
    "    return daily_ambiguity_metric, interval_std, risk\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    # Read the CSV with the expected columns\n",
    "    df = pd.read_csv(file_path, usecols=['datetime_nano', 'SSE.000300.close'])\n",
    "    df.rename(columns={'SSE.000300.close': 'close'}, inplace=True)\n",
    "    \n",
    "    # Convert 'datetime_nano' to a datetime object.\n",
    "    # First interpret as UTC date, then convert to local time (Asia/Shanghai), then floor to minute.\n",
    "    df['datetime'] = (pd.to_datetime(df['datetime_nano'], utc=True)\n",
    "                        .dt.tz_convert('Asia/Shanghai')\n",
    "                        .dt.floor('min')\n",
    "                        .dt.tz_localize(None))\n",
    "    df.drop('datetime_nano', axis=1, inplace=True)\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df.sort_values('datetime', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to use the code\n",
    "    # Note: You need to have a DataFrame 'df' with 'datetime' and 'close' columns\n",
    "    file_path = '../../data/SSE.000300.csv'  # Ensure the path is correct\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    # Set parameters\n",
    "    specific_date = '2018-01-09'\n",
    "    window_size = 5  # 5-day window\n",
    "    num_bins = 50    # 20 bins for histogram\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ambiguity_metric, interval_std, risk = calculate_ambiguity_and_risk(\n",
    "        df, \n",
    "        specific_date, \n",
    "        window_size=window_size, \n",
    "        num_bins=num_bins\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Ambiguity Metric on {specific_date}: {ambiguity_metric}\")\n",
    "    print(f\"Risk (Standard Deviation of Daily Returns) on {specific_date}: {risk}\")\n",
    "    #print(\"Standard deviation across bins:\")\n",
    "    #print(interval_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
