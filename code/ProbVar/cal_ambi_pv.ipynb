{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity Metric on 2018-01-09: 0.005197856487538408\n",
      "Risk (Standard Deviation of Daily Returns) on 2018-01-09: 0.0017383918259038763\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "from typing import Tuple, List, Union  # For type hints\n",
    "from datetime import date  # For date handling\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the input DataFrame by calculating returns and extracting dates.\n",
    "    This function handles the initial data preprocessing steps.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'datetime' and 'close' columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'return' and 'date' columns, sorted by datetime\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime column to pandas datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Sort data chronologically to ensure correct return calculations\n",
    "    df.sort_values('datetime', inplace=True)\n",
    "    \n",
    "    # Calculate percentage returns (price changes)\n",
    "    # pct_change() computes the percentage change between consecutive elements\n",
    "    df['return'] = df['close'].pct_change()\n",
    "    \n",
    "    # Extract date component from datetime for daily grouping\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    \n",
    "    # Remove rows with NaN returns (first row will have NaN as there's no previous price)\n",
    "    return df.dropna(subset=['return'])\n",
    "\n",
    "def get_window_dates(df: pd.DataFrame, specific_date: Union[str, date], \n",
    "                     window_size: int) -> List[date]:\n",
    "    \"\"\"\n",
    "    Get the dates for the rolling window analysis.\n",
    "    This function ensures we have enough data for the analysis and returns\n",
    "    the appropriate date range.\n",
    "    \n",
    "    Args:\n",
    "        df: Prepared DataFrame with 'date' column\n",
    "        specific_date: Target date for analysis\n",
    "        window_size: Number of days in rolling window\n",
    "        \n",
    "    Returns:\n",
    "        List of dates in the rolling window\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If specific_date not found or insufficient data\n",
    "    \"\"\"\n",
    "    # Convert specific_date to datetime.date object if it's a string\n",
    "    specific_date = pd.to_datetime(specific_date).date()\n",
    "    \n",
    "    # Get unique dates and sort them chronologically\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # Validate that the specific date exists in our data\n",
    "    if specific_date not in unique_dates:\n",
    "        raise ValueError(\"Specific date not found in the data.\")\n",
    "    \n",
    "    # Find the index of our target date\n",
    "    date_index = unique_dates.index(specific_date)\n",
    "    \n",
    "    # Check if we have enough historical data for the window\n",
    "    if date_index < window_size - 1:\n",
    "        raise ValueError(\"Not enough data for the specified window size.\")\n",
    "    \n",
    "    # Return the window of dates (including the specific date)\n",
    "    return unique_dates[date_index - window_size + 1 : date_index + 1]\n",
    "\n",
    "def calculate_daily_probabilities(window_data: pd.DataFrame, \n",
    "                                num_bins: int) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate probability distributions for each day in the window.\n",
    "    This function creates histograms of returns and converts them to probability distributions.\n",
    "    \n",
    "    Args:\n",
    "        window_data: DataFrame containing window data with 'return' and 'date' columns\n",
    "        num_bins: Number of bins for histogram (determines granularity of distribution)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - DataFrame of daily probability distributions\n",
    "        - Array of bin edges used for the histograms\n",
    "    \"\"\"\n",
    "    # Find the range of returns in our window\n",
    "    min_return = window_data['return'].min()\n",
    "    max_return = window_data['return'].max()\n",
    "    \n",
    "    # Create evenly spaced bins across the return range\n",
    "    bins = np.linspace(min_return, max_return, num_bins + 1)\n",
    "    \n",
    "    # Calculate probability distribution for each day\n",
    "    daily_probs = []\n",
    "    for date in window_data['date'].unique():\n",
    "        # Get returns for this specific day\n",
    "        daily_returns = window_data[window_data['date'] == date]['return']\n",
    "        \n",
    "        # Create histogram of returns\n",
    "        counts, _ = np.histogram(daily_returns, bins=bins)\n",
    "        \n",
    "        # Convert counts to probabilities\n",
    "        # If there are no returns (counts.sum() == 0), use zeros\n",
    "        probabilities = counts / counts.sum() if counts.sum() > 0 else np.zeros(num_bins)\n",
    "        daily_probs.append(probabilities)\n",
    "    \n",
    "    # Create DataFrame with dates as index and probabilities as columns\n",
    "    return pd.DataFrame(daily_probs, index=window_data['date'].unique()), bins\n",
    "\n",
    "def calculate_risk(df: pd.DataFrame, window_dates: List[date]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate risk as standard deviation of daily returns.\n",
    "    This is a traditional measure of volatility/risk in financial markets.\n",
    "    \n",
    "    Args:\n",
    "        df: Original DataFrame with price data\n",
    "        window_dates: List of dates in the analysis window\n",
    "        \n",
    "    Returns:\n",
    "        Risk metric (standard deviation of daily returns)\n",
    "    \"\"\"\n",
    "    # Get the last price of each day\n",
    "    daily_close = df.groupby('date')['close'].last()\n",
    "    \n",
    "    # Calculate daily returns\n",
    "    daily_returns = daily_close.pct_change().dropna()\n",
    "    \n",
    "    # Filter returns for our window\n",
    "    window_daily_returns = daily_returns.loc[window_dates]\n",
    "    \n",
    "    # Calculate standard deviation of daily returns\n",
    "    return window_daily_returns.std()\n",
    "\n",
    "def calculate_ambiguity_and_risk(df: pd.DataFrame, \n",
    "                               specific_date: Union[str, date],\n",
    "                               window_size: int = 5, \n",
    "                               num_bins: int = 20) -> Tuple[float, pd.Series, float]:\n",
    "    \"\"\"\n",
    "    Main function to calculate ambiguity and risk metrics for a specific date.\n",
    "    This function orchestrates the entire analysis process.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'datetime' and 'close' columns\n",
    "        specific_date: Target date for analysis\n",
    "        window_size: Number of days in rolling window (default: 5)\n",
    "        num_bins: Number of bins for histogram (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - ambiguity_metric: Average standard deviation across bins\n",
    "        - interval_std: Standard deviations for each bin\n",
    "        - risk: Standard deviation of daily returns\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare the data\n",
    "    df = prepare_data(df)\n",
    "    \n",
    "    # Step 2: Get the dates for our analysis window\n",
    "    window_dates = get_window_dates(df, specific_date, window_size)\n",
    "    \n",
    "    # Step 3: Filter data for our window\n",
    "    window_data = df[df['date'].isin(window_dates)].copy()\n",
    "    \n",
    "    # Step 4: Calculate probability distributions\n",
    "    prob_df, _ = calculate_daily_probabilities(window_data, num_bins)\n",
    "    \n",
    "    # Step 5: Calculate ambiguity metrics\n",
    "    # Standard deviation across days for each bin\n",
    "    interval_std = prob_df.std(axis=0)\n",
    "    # Average standard deviation across all bins\n",
    "    daily_ambiguity_metric = interval_std.mean()\n",
    "    \n",
    "    # Step 6: Calculate risk\n",
    "    risk = calculate_risk(df, window_dates)\n",
    "    \n",
    "    return daily_ambiguity_metric, interval_std, risk\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    # Read the CSV with the expected columns\n",
    "    df = pd.read_csv(file_path, usecols=['datetime_nano', 'SSE.000300.close'])\n",
    "    df.rename(columns={'SSE.000300.close': 'close'}, inplace=True)\n",
    "    \n",
    "    # Convert 'datetime_nano' to a datetime object.\n",
    "    # First interpret as UTC date, then convert to local time (Asia/Shanghai), then floor to minute.\n",
    "    df['datetime'] = (pd.to_datetime(df['datetime_nano'], utc=True)\n",
    "                        .dt.tz_convert('Asia/Shanghai')\n",
    "                        .dt.floor('min')\n",
    "                        .dt.tz_localize(None))\n",
    "    df.drop('datetime_nano', axis=1, inplace=True)\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df.sort_values('datetime', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to use the code\n",
    "    # Note: You need to have a DataFrame 'df' with 'datetime' and 'close' columns\n",
    "    file_path = '../../data/SSE.000300.csv'  # Ensure the path is correct\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    # Set parameters\n",
    "    specific_date = '2018-01-09'\n",
    "    window_size = 5  # 5-day window\n",
    "    num_bins = 50    # 20 bins for histogram\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ambiguity_metric, interval_std, risk = calculate_ambiguity_and_risk(\n",
    "        df, \n",
    "        specific_date, \n",
    "        window_size=window_size, \n",
    "        num_bins=num_bins\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Ambiguity Metric on {specific_date}: {ambiguity_metric}\")\n",
    "    print(f\"Risk (Standard Deviation of Daily Returns) on {specific_date}: {risk}\")\n",
    "    #print(\"Standard deviation across bins:\")\n",
    "    #print(interval_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics...\n"
     ]
    }
   ],
   "source": [
    "# Import additional required libraries for visualization and analysis\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For statistical visualizations\n",
    "from scipy import stats  # For statistical tests\n",
    "from typing import Tuple, List, Union  # For type hints\n",
    "from datetime import date\n",
    "\n",
    "def calculate_metrics_for_date_range(df: pd.DataFrame, \n",
    "                                   start_date: str,\n",
    "                                   window_size: int = 5,\n",
    "                                   num_bins: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate ambiguity and risk metrics for a range of dates.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'datetime' and 'close' columns\n",
    "        start_date: Start date for analysis (format: 'YYYY-MM-DD')\n",
    "        window_size: Number of days in rolling window\n",
    "        num_bins: Number of bins for histogram\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: date, ambiguity_metric, risk\n",
    "    \"\"\"\n",
    "    # Prepare the data using existing function\n",
    "    df = prepare_data(df)\n",
    "    \n",
    "    # Get unique dates and sort them\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # Convert start_date to datetime.date\n",
    "    start_date = pd.to_datetime(start_date).date()\n",
    "    \n",
    "    # Filter dates from start_date onwards\n",
    "    analysis_dates = [d for d in unique_dates if d >= start_date]\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # Calculate metrics for each date\n",
    "    for current_date in analysis_dates:\n",
    "        try:\n",
    "            # Use existing function to calculate metrics\n",
    "            ambiguity_metric, _, risk = calculate_ambiguity_and_risk(\n",
    "                df,\n",
    "                current_date,\n",
    "                window_size=window_size,\n",
    "                num_bins=num_bins\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'date': current_date,\n",
    "                'ambiguity_metric': ambiguity_metric,\n",
    "                'risk': risk\n",
    "            })\n",
    "            \n",
    "        except ValueError as e:\n",
    "            # Skip dates where we don't have enough data\n",
    "            print(f\"Skipping {current_date}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_correlation(results_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and visualize the correlation between ambiguity and risk metrics.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing date, ambiguity_metric, and risk columns\n",
    "    \"\"\"\n",
    "    # Calculate correlation coefficient\n",
    "    correlation = results_df['ambiguity_metric'].corr(results_df['risk'])\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Time series plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(results_df['date'], results_df['ambiguity_metric'], label='Ambiguity')\n",
    "    plt.plot(results_df['date'], results_df['risk'], label='Risk')\n",
    "    plt.title('Ambiguity and Risk Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. Scatter plot with regression line\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.regplot(data=results_df, x='ambiguity_metric', y='risk')\n",
    "    plt.title(f'Correlation: {correlation:.3f}')\n",
    "    plt.xlabel('Ambiguity Metric')\n",
    "    plt.ylabel('Risk')\n",
    "    \n",
    "    # 3. Histogram of ambiguity metric\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(results_df['ambiguity_metric'], kde=True)\n",
    "    plt.title('Distribution of Ambiguity Metric')\n",
    "    plt.xlabel('Ambiguity Metric')\n",
    "    \n",
    "    # 4. Histogram of risk\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.histplot(results_df['risk'], kde=True)\n",
    "    plt.title('Distribution of Risk')\n",
    "    plt.xlabel('Risk')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ambiguity_risk_analysis.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(results_df.describe())\n",
    "    \n",
    "    # Print correlation analysis\n",
    "    print(\"\\nCorrelation Analysis:\")\n",
    "    print(f\"Pearson Correlation Coefficient: {correlation:.3f}\")\n",
    "    \n",
    "    # Perform statistical test\n",
    "    t_stat, p_value = stats.pearsonr(results_df['ambiguity_metric'], results_df['risk'])\n",
    "    print(f\"P-value: {p_value:.3e}\")\n",
    "    print(f\"T-statistic: {t_stat:.3f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    file_path = '../../data/SSE.000300.csv'\n",
    "    df = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Set parameters\n",
    "    start_date = '2018-01-09'\n",
    "    window_size = 5\n",
    "    num_bins = 50\n",
    "    \n",
    "    # Calculate metrics for the date range\n",
    "    print(\"Calculating metrics...\")\n",
    "    results_df = calculate_metrics_for_date_range(\n",
    "        df,\n",
    "        start_date,\n",
    "        window_size=window_size,\n",
    "        num_bins=num_bins\n",
    "    )\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('daily_ambiguity_risk_metrics.csv', index=False)\n",
    "    print(\"\\nResults saved to '300_daily_ambiguity_risk_metrics.csv'\")\n",
    "    \n",
    "    # Analyze correlation and create visualizations\n",
    "    print(\"\\nAnalyzing correlation...\")\n",
    "    analyze_correlation(results_df)\n",
    "    \n",
    "    # Display first few rows of results\n",
    "    print(\"\\nFirst few rows of results:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    # Calculate rolling correlation (30-day window)\n",
    "    rolling_corr = results_df['ambiguity_metric'].rolling(window=30).corr(results_df['risk'])\n",
    "    \n",
    "    # Plot rolling correlation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results_df['date'], rolling_corr)\n",
    "    plt.title('30-Day Rolling Correlation between Ambiguity and Risk')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('rolling_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nAnalysis complete. Check the generated CSV and PNG files for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to daily_ambiguity_var_para.csv\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define parameter ranges\n",
    "window_sizes = list(range(5, 21, 3))   # 5, 8, 11, 14, 17, 20\n",
    "num_bins_list = list(range(20, 101, 10))  # 20, 30, ..., 100\n",
    "\n",
    "# Prepare data\n",
    "file_path = '../../data/SSE.000300.csv'\n",
    "df = load_and_prepare_data(file_path)\n",
    "df = prepare_data(df)\n",
    "unique_dates = sorted(df['date'].unique())\n",
    "\n",
    "# Only use dates where all window sizes are possible\n",
    "min_window = min(window_sizes)\n",
    "analysis_dates = unique_dates[min_window-1:]\n",
    "\n",
    "# Prepare result storage\n",
    "results = {'date': analysis_dates}\n",
    "\n",
    "# For each parameter combination, calculate ambiguity metric for all dates\n",
    "for window_size, num_bins in itertools.product(window_sizes, num_bins_list):\n",
    "    col_name = f\"{window_size}d{num_bins}b\"\n",
    "    metrics = []\n",
    "    for current_date in analysis_dates:\n",
    "        try:\n",
    "            ambiguity_metric, _, _ = calculate_ambiguity_and_risk(\n",
    "                df, current_date, window_size=window_size, num_bins=num_bins\n",
    "            )\n",
    "        except Exception:\n",
    "            ambiguity_metric = float('nan')\n",
    "        metrics.append(ambiguity_metric)\n",
    "    results[col_name] = metrics\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('daily_ambiguity_var_para.csv', index=False)\n",
    "print(\"Saved to daily_ambiguity_var_para.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
